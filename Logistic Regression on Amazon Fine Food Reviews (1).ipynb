{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from string import punctuation\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from itertools import chain\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline \n",
    "import pickle\n",
    "def savetofile(obj,filename):\n",
    "    pickle.dump(obj,open(filename+\".p\",\"wb\"), protocol=4)\n",
    "def openfromfile(filename):\n",
    "    temp = pickle.load(open(filename+\".p\",\"rb\"))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Original Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('desktop/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the cleaned and processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.shape\n",
    "final_data['Score'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score as positive/negative -> 0/1\n",
    "def polarity(x):\n",
    "    if x == \"Positive\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "final_data[\"Score\"] = final_data[\"Score\"].map(polarity) #Map all the scores as the function polarity i.e. positive or negative\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Sample Data\n",
    "n_samples = 25000\n",
    "df_sample = final_data.sample(n_samples)\n",
    "\n",
    "###Sorting as we want according to time series\n",
    "df_sample.sort_values('Time',inplace=True) \n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving 25000 samples in disk to as to test to test on the same sample for each of all Algo\n",
    "savetofile(df_sample,\"sample_25000_knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening from samples from file\n",
    "df_sample = openfromfile(\"sample_25000_knn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model on Reviews using Different Vectorizing Techniques in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "A commonly used model in methods of Text Classification. As part of the BOW model, a piece of text (sentence or a document) is represented as a bag or multiset of words, disregarding grammar and even word order and the frequency or occurrence of each word is used as a feature for training a classifier.\n",
    "OR\n",
    "Simply,Converting a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Breaking into Train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample['CleanedText'].values,df_sample['Score'].values,test_size=0.3,shuffle=False)\n",
    "\n",
    "#Text -> Uni gram Vectors\n",
    "uni_gram = CountVectorizer() \n",
    "X_train = uni_gram.fit_transform(X_train).astype(str)\n",
    "#Normalize Data\n",
    "X_train = preprocessing.normalize(X_train)\n",
    "print(\"Train Data Size: \",X_train.shape)\n",
    "X_test = uni_gram.transform(X_test)\n",
    "#Normalize Data\n",
    "X_test = preprocessing.normalize(X_test)\n",
    "print(\"Test Data Size: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To show how Time Series Split splits the data\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "for train, cv in tscv.split(X_train):\n",
    "#     print(\"%s %s\" % (train, cv))\n",
    "    print(X_train[train].shape, X_train[cv].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best \"C\" or \"1/lambda\" and regularizer [ L1 or L2 ] using Forward Chaining Cross Validation or Time Series CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "             'penalty':['l1','l2']} \n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_uni\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_vs_c(gsv):\n",
    "    x1=[]\n",
    "    y1=[]\n",
    "    x2=[]\n",
    "    y2=[]\n",
    "    for a in gsv.grid_scores_:\n",
    "        if (a[0]['penalty']) == 'l1':\n",
    "            y1.append(1-a[1])\n",
    "            x1.append(a[0]['C'])\n",
    "        else:\n",
    "            y2.append(1-a[1])\n",
    "            x2.append(a[0]['C'])\n",
    "    plt.xlim(-10,1010)\n",
    "    plt.ylim(0,0.2)\n",
    "    plt.xlabel(\"C\",fontsize=15)\n",
    "    plt.ylabel(\"Misclassification Error\")\n",
    "    plt.title('Misclassification Error v/s C')\n",
    "    plt.plot(x1,y1,'b',label=\"L1\")\n",
    "    plt.plot(x2,y2,'r',label=\"L2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "gsv = openfromfile(\"Log Reg/gsv_uni\")\n",
    "plot_error_vs_c(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing how sparsity increases as we increase lambda or decrease C when L1 Regularizer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1000, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 100, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 0.1, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 0.01, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see how drastically the sparsity increases from 59300 non-zero weights(@ C=1000) to only 76 non-zero weights(@ C=0.01) when we use L1 Regularization\n",
    "\n",
    "### Using Randomized Search CV to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = { 'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "              'penalty':['l1','l2']}\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = RandomizedSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_uni_r\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 5, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot Misclassification error against C\n",
    "def plot_error_vs_c_r(gsv):\n",
    "    x1=[]\n",
    "    y1=[]\n",
    "    x2=[]\n",
    "    y2=[]\n",
    "    for a in gsv.grid_scores_:\n",
    "        if (a[0]['penalty']) == 'l1':\n",
    "            y1.append(1-a[1])\n",
    "            x1.append(a[0]['C'])\n",
    "        else:\n",
    "            y2.append(1-a[1])\n",
    "            x2.append(a[0]['C'])\n",
    "\n",
    "    ind1 = np.argsort(x1)\n",
    "    x1=np.array(x1)\n",
    "    y1=np.array(y1)\n",
    "    ind2 = np.argsort(x2)\n",
    "    x2=np.array(x2)\n",
    "    y2=np.array(y2)\n",
    "    plt.xlim(-10,1010)\n",
    "    plt.ylim(0,0.2)\n",
    "    plt.xlabel(\"C\",fontsize=15)\n",
    "    plt.ylabel(\"Misclassification Error\")\n",
    "    plt.title('Misclassification Error v/s C')\n",
    "    plt.plot(x1[ind1],y1[ind1],'b',label=\"L1\")\n",
    "    plt.plot(x2[ind2],y2[ind2],'r',label=\"L2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "gsv = openfromfile(\"Log Reg/gsv_uni_r\")\n",
    "plot_error_vs_c_r(gsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights before adding random noise\n",
    "weights1 = find(clf.coef_[0])[2]\n",
    "print(weights1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train\n",
    "#Random noise\n",
    "epsilon = np.random.uniform(low=-0.0001, high=0.0001, size=(find(X_train_t)[0].size,))\n",
    "#Getting the postions(row and column) and value of non-zero datapoints \n",
    "a,b,c = find(X_train_t)\n",
    "\n",
    "#Introducing random noise to non-zero datapoints\n",
    "X_train_t[a,b] = epsilon + X_train_t[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on train data having random noise\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train_t,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights after adding random noise\n",
    "weights2 = find(clf.coef_[0])[2]\n",
    "print(weights2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_diff = (abs(weights1 - weights2)/weights1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights_diff[np.where(weights_diff > 30)].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42 features have weight changes greater than 30%. Hence the features are multicollinear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(uni_gram,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Breaking into Train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample['CleanedText'].values,df_sample['Score'].values,test_size=0.3,shuffle=False)\n",
    "\n",
    "#taking one words and two consecutive words together\n",
    "bi_gram = CountVectorizer(ngram_range=(1,2)) \n",
    "X_train = bi_gram.fit_transform(X_train)\n",
    "#Normalize Data\n",
    "X_train = preprocessing.normalize(X_train)\n",
    "print(\"Train Data Size: \",X_train.shape)\n",
    "X_test = bi_gram.transform(X_test)\n",
    "#Normalize Data\n",
    "X_test = preprocessing.normalize(X_test)\n",
    "print(\"Test Data Size: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "             'penalty':['l1','l2']} \n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_bi\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_bi\")\n",
    "plot_error_vs_c(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 100, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing how sparsity increases as we increase lambda or decrease C when L1 Regularizer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1000, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 100, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 0.1, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 0.01, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see how drastically the sparsity increases from 52309 non-zero weights(@ C=1000) to only 55 non-zero weights(@ C=0.01) when we use L1 Regularization\n",
    "\n",
    "## Using Randomized Search CV to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "              'penalty':['l1','l2']}\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = RandomizedSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_bi_r\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_bi_r\")\n",
    "plot_error_vs_c_r(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 100, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 500, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights before adding random noise\n",
    "weights1 = find(clf.coef_[0])[2]\n",
    "print(weights1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train\n",
    "#Random noise\n",
    "epsilon = np.random.uniform(low=-0.0001, high=0.0001, size=(find(X_train_t)[0].size,))\n",
    "#Getting the postions(row and column) and value of non-zero datapoints \n",
    "a,b,c = find(X_train_t)\n",
    "\n",
    "#Introducing random noise to non-zero datapoints\n",
    "X_train_t[a,b] = epsilon + X_train_t[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on train data having random noise\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train_t,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights after adding random noise\n",
    "weights2 = find(clf.coef_[0])[2]\n",
    "print(weights2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_diff = (abs(weights1 - weights2)/weights1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights_diff[np.where(weights_diff > 30)].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 526050 features have weight changes greater than 30%. Hence the features are multicollinear\n",
    "\n",
    "## Feature Importance[Top 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(bi_gram,clf)\n",
    "#Code Reference:https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Breaking into Train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample['CleanedText'].values,df_sample['Score'].values,test_size=0.3,shuffle=False)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2)) #Using bi-grams\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "#Normalize Data\n",
    "X_train = preprocessing.normalize(X_train)\n",
    "print(\"Train Data Size: \",X_train.shape)\n",
    "X_test = tfidf.transform(X_test)\n",
    "#Normalize Data\n",
    "X_test = preprocessing.normalize(X_test)\n",
    "print(\"Test Data Size: \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "             'penalty':['l1','l2']} \n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_tfidf\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_tfidf\")\n",
    "plot_error_vs_c(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 5, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing how sparsity increases as we increase lambda or decrease C when L1 Regularizer is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1000, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 100, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 0.1, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 0.01, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see how drastically the sparsity increases from 62771 non-zero weights(@ C=1000) to only 20 non-zero weights(@ C=0.01) when we use L1 Regularization\n",
    "\n",
    "## Using Randomized Search CV to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "              ,'penalty':['l1','l2']}\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = RandomizedSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_tfidf_r\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_tfidf_r\")\n",
    "plot_error_vs_c_r(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 5, penalty= 'l1')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 500, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights before adding random noise\n",
    "weights1 = find(clf.coef_[0])[2]\n",
    "print(weights1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights1[weights1<=0.0001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train\n",
    "#Random noise\n",
    "epsilon = np.random.uniform(low=-0.0001, high=0.0001, size=(find(X_train_t)[0].size,))\n",
    "#Getting the postions(row and column) and value of non-zero datapoints \n",
    "a,b,c = find(X_train_t)\n",
    "\n",
    "#Introducing random noise to non-zero datapoints\n",
    "X_train_t[a,b] = epsilon + X_train_t[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on train data having random noise\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train_t,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights after adding random noise\n",
    "weights2 = find(clf.coef_[0])[2]\n",
    "print(weights2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_diff = (abs(weights1 - weights2)/weights1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights_diff[np.where(weights_diff > 30)].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 531188 features have weight changes greater than 30%. Hence the features are multicollinear\n",
    "\n",
    "## Feature Importance[Top 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=25):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    print(\"\\t\\t\\tPositive\\t\\t\\t\\t\\t\\tNegative\")\n",
    "    print(\"________________________________________________________________________________________________\")\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(tfidf,clf)\n",
    "#Code Reference:https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim\n",
    "Gensim is a robust open-source vector space modeling and topic modeling toolkit implemented in Python. It uses NumPy, SciPy and optionally Cython for performance. Gensim is specifically designed to handle large text collections, using data streaming and efficient incremental algorithms, which differentiates it from most other scientific software packages that only target batch and in-memory processing.\n",
    "\n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Loading the model from file in the disk\n",
    "w2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocub = w2vec_model.wv.vocab\n",
    "len(w2v_vocub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Word2Vec\n",
    "One of the most naive but good ways to convert a sentence into a vector\n",
    "Convert all the words to vectors and then just take the avg of the vectors the resulting vector represent the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "avg_vec_google = [] #List to store all the avg w2vec's \n",
    "# no_datapoints = 364170\n",
    "# sample_cols = random.sample(range(1, no_datapoints), 20001)\n",
    "for sent in df_sample['CleanedText_NoStem']:\n",
    "    cnt = 0 #to count no of words in each reviews\n",
    "    sent_vec = np.zeros(300) #Initializing with zeroes\n",
    "#     print(\"sent:\",sent) \n",
    "    sent = sent.decode(\"utf-8\") \n",
    "    for word in sent.split():\n",
    "        try:\n",
    "#             print(word)\n",
    "            wvec = w2vec_model.wv[word] #Vector of each using w2v model\n",
    "#             print(\"wvec:\",wvec)\n",
    "            sent_vec += wvec #Adding the vectors\n",
    "#             print(\"sent_vec:\",sent_vec)\n",
    "            cnt += 1\n",
    "        except: \n",
    "            pass #When the word is not in the dictionary then do nothing  \n",
    "#     print(sent_vec)\n",
    "    sent_vec /= cnt #Taking average of vectors sum of the particular review\n",
    "#     print(\"avg_vec:\",sent_vec)\n",
    "    avg_vec_google.append(sent_vec) #Storing the avg w2vec's for each review\n",
    "#     print(\"*******************************************************************\")\n",
    "# print(avg_vec_google)\n",
    "avg_vec_google = np.array(avg_vec_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(avg_vec_google).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.any(np.isnan(avg_vec_google), axis=1)\n",
    "# print(mask)\n",
    "avg_vec_google_new = avg_vec_google[mask]\n",
    "df_sample_new = df_sample['Score'][mask]\n",
    "print(avg_vec_google_new.shape)\n",
    "print(df_sample_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "#Normalizing the data\n",
    "avg_vec_norm = preprocessing.normalize(avg_vec_google_new)\n",
    "\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(avg_vec_norm,df_sample_new.values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "             'penalty':['l1','l2']} \n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_w2v\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_w2v\")\n",
    "plot_error_vs_c(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 1000, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Randomized Search CV to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C': [1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "              ,'penalty':['l1','l2']}\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = RandomizedSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_w2v_r\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_w2v_r\")\n",
    "plot_error_vs_c_r(gsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights before adding random noise\n",
    "weights1 = find(clf.coef_[0])[2]\n",
    "print(weights1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train\n",
    "#Random noise\n",
    "epsilon = np.random.uniform(low=-0.01, high=0.01, size=(find(X_train_t)[0].size,))\n",
    "#Getting the postions(row and column) and value of non-zero datapoints \n",
    "a,b,c = find(X_train_t)\n",
    "\n",
    "#Introducing random noise to non-zero datapoints\n",
    "X_train_t[a,b] = epsilon + X_train_t[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on train data having random noise\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train_t,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights after adding random noise\n",
    "weights2 = find(clf.coef_[0])[2]\n",
    "print(weights2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_diff = (abs(weights1 - weights2)/weights1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights_diff[np.where(weights_diff > 30)].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 features have weight changes greater than 30%. Hence the features are multicollinear\n",
    "\n",
    "## Tf-idf W2Vec\n",
    "1. Another way to covert sentence into vectors\n",
    "2. Take weighted sum of the vectors divided by the sum of all the tfidf's\n",
    "i.e. (tfidf(word) x w2v(word))/sum(tfidf's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###tf-idf with No Stemming\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "#Taking sample of only 25k points as it takes a huge amount of time ot compute \n",
    "n_samples = 25000\n",
    "df_sample_new = df_sample.sample(n_samples)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample_new['CleanedText_NoStem'].values,df_sample_new['Score'].values,test_size=0.3,shuffle=False)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2)) #Using bi-grams\n",
    "tfidf_vec_train = tfidf.fit_transform(X_train)\n",
    "tfidf_vec_test = tfidf.transform(X_test)\n",
    "print(tfidf_vec_train.shape)\n",
    "print(tfidf_vec_test.shape)\n",
    "\n",
    "#Concatenating sparse matrix vertically\n",
    "tfidf_vec_new = vstack((tfidf_vec_train,tfidf_vec_test))\n",
    "print(tfidf_vec.shape)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savetofile(df_sample_new,\"df_sample_new_tfidfw2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "tfidf_w2v_vec_google = []\n",
    "review = 0\n",
    "\n",
    "for sent in df_sample_new['CleanedText_NoStem'].values:\n",
    "    cnt = 0 \n",
    "    weighted_sum  = 0\n",
    "    sent_vec = np.zeros(300)\n",
    "    sent = sent.decode(\"utf-8\") \n",
    "    for word in sent.split():\n",
    "        try:\n",
    "#             print(word)\n",
    "            wvec = w2vec_model.wv[word] #Vector of each using w2v model\n",
    "#             print(\"w2vec:\",wvec)\n",
    "#             print(\"tfidf:\",tfidf_vec_new[review,features.index(word)])\n",
    "            tfidf_vec = tfidf_vec_new[review,features.index(word)]\n",
    "            sent_vec += (wvec * tfidf_vec)\n",
    "            weighted_sum += tfidf_vec\n",
    "        except:\n",
    "#             print(review)\n",
    "            pass\n",
    "    sent_vec /= weighted_sum\n",
    "#     print(sent_vec)\n",
    "    tfidf_w2v_vec_google.append(sent_vec)\n",
    "    review += 1\n",
    "tfidf_w2v_vec_google = np.array(tfidf_w2v_vec_google)\n",
    "savetofile(tfidf_w2v_vec_google,\"tfidf_w2v_vec_google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precomputed File\n",
    "tfidf_w2v_vec_google = openfromfile(\"tfidf_w2v_vec_google\")\n",
    "#Loading the same samples as using precomuted file\n",
    "df_sample_new = openfromfile(\"df_sample_new_tfidfw2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidfw2v_vecs_norm = preprocessing.normalize(tfidf_w2v_vec_google)\n",
    "\n",
    "#Not shuffling the data as we want it on time basis\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidfw2v_vecs_norm,df_sample_new['Score'].values,test_size=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "             'penalty':['l1','l2']} \n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = GridSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_w2vtfidf\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_w2vtfidf\")\n",
    "plot_error_vs_c(gsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Randomized Search CV to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "#params we need to try on classifier\n",
    "param_grid = {'C':[1000,500,100,50,10,5,1,0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001],\n",
    "              'penalty':['l1','l2']}\n",
    "tscv = TimeSeriesSplit(n_splits=10) #For time based splitting\n",
    "gsv = RandomizedSearchCV(clf,param_grid,cv=tscv,verbose=1)\n",
    "gsv.fit(X_train,y_train)\n",
    "savetofile(gsv,\"Log Reg/gsv_w2vtfidf_r\")\n",
    "print(\"Best HyperParameter: \",gsv.best_params_)\n",
    "print(\"Best Accuracy: %.2f%%\"%(gsv.best_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = openfromfile(\"Log Reg/gsv_w2vtfidf_r\")\n",
    "plot_error_vs_c_r(gsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Accuracy on Test data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 5, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision on test set: %0.3f\"%(precision_score(y_test, y_pred)))\n",
    "print(\"Recall on test set: %0.3f\"%(recall_score(y_test, y_pred)))\n",
    "print(\"F1-Score on test set: %0.3f\"%(f1_score(y_test, y_pred)))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))\n",
    "print(\"Confusion Matrix of test set:\\n [ [TN  FP]\\n [FN TP] ]\\n\")\n",
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), range(2),range(2))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 5, penalty= 'l2')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights before adding random noise\n",
    "weights1 = find(clf.coef_[0])[2]\n",
    "print(weights1[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train\n",
    "#Random noise\n",
    "epsilon = np.random.uniform(low=-0.0001, high=0.0001, size=(find(X_train_t)[0].size,))\n",
    "#Getting the postions(row and column) and value of non-zero datapoints \n",
    "a,b,c = find(X_train_t)\n",
    "\n",
    "#Introducing random noise to non-zero datapoints\n",
    "X_train_t[a,b] = epsilon + X_train_t[a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training on train data having random noise\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C= 10, penalty= 'l2')\n",
    "clf.fit(X_train_t,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy on test set: %0.3f%%\"%(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Non Zero weights:\",np.count_nonzero(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find\n",
    "#Weights after adding random noise\n",
    "weights2 = find(clf.coef_[0])[2]\n",
    "print(weights2[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_diff = (abs(weights2 - weights1)/weights1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights_diff[np.where(weights_diff > 30)].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24 features have weight changes greater than 30%. Hence the features are multicollinear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "1. Features are multi-collinear i.e. they are co-related\n",
    "2. Bigram Featurization performs best with accuracy of 93.704 and F1-Score of 0.808\n",
    "3. Sparsity increases as we increase lambda or decrease C when L1 Regularizer is used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
